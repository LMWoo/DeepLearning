{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4556, 1.0479, 0.8981, 0.9442])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import third_party_cpp as test_cpp\n",
    "\n",
    "x = torch.randn(4)\n",
    "y = torch.randn(4)\n",
    "print(test_cpp.sigmoid_add(x, y))\n",
    "\n",
    "shape = test_cpp.Shape(5, 3)\n",
    "cArray = test_cpp.NdArray(shape)\n",
    "ret = cArray.ones()\n",
    "cArray.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1, 2], [2, 3]])\n",
    "y = np.array([[2, 3], [1, 2]])\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(x @ y)\n",
    "print(y @ x)\n",
    "print(np.dot(x, y))\n",
    "print(np.dot(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'third_party_cpp' has no attribute 'toNumCpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0ad035553e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mncpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoNumCpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mncpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoNumCpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'third_party_cpp' has no attribute 'toNumCpp'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import third_party_cpp as ncpp\n",
    "\n",
    "x = np.array([[1, 2], [2, 3]])\n",
    "y = np.array([[2, 3], [1, 2]])\n",
    "\n",
    "x = ncpp.toNumCpp(x)\n",
    "y = ncpp.toNumCpp(y)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "seq_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "data_path = \"/media/lee/ESD-ISO/script_test/Data/mnist/\"\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "class My_RNN(object):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        self.lr = learning_rate\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.U = xavier_init(hidden_size, input_size, fc=True) # rnn input parameters\n",
    "        # self.W = xavier_init(hidden_size, hidden_size, fc=True) # rnn hidden parameters\n",
    "        # self.V = xavier_init(hidden_size, hidden_size, fc=True) # rnn output parameters4\n",
    "        self.U = np.zeros((hidden_size, input_size))\n",
    "        self.W = np.zeros((hidden_size, hidden_size)) # rnn hidden parameters\n",
    "        self.V = np.zeros((hidden_size, hidden_size)) # rnn output parameters\n",
    "\n",
    "        self.b = np.zeros((hidden_size, 1)) # rnn input parameters\n",
    "        self.c = np.zeros((hidden_size, 1)) # rnn output parameters\n",
    "        \n",
    "        self.FC_W = xavier_init(num_classes, hidden_size, fc=True) # fc parameters\n",
    "        self.fc_b = np.zeros((num_classes, 1)) # fc parameters\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "        self.mFC_W = np.zeros_like(self.FC_W)\n",
    "        self.mfc_b = np.zeros_like(self.fc_b)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.A = {}\n",
    "        self.S = {}\n",
    "        self.O = {}\n",
    "        self.FC_O = {}\n",
    "        \n",
    "    def forward(self, x, hprev):\n",
    "        self.S[-1] = np.copy(hprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = x[t].T\n",
    "            self.A[t] = self.U @ self.X[t] + self.W @ self.S[t - 1] + self.b\n",
    "            self.S[t] = np.tanh(self.A[t])\n",
    "            self.O[t] = self.V @ self.S[t] + self.c # (hidden, hidden) @ (hidden, 1) + (hidden, 1)\n",
    "        \n",
    "        self.FC_O = self.FC_W @ self.O[self.seq_length - 1] + self.fc_b # (classes, hidden) @ (hidden, 1) + (classes, 1)\n",
    "        \n",
    "        return self.FC_O # (classes, 1)\n",
    "    \n",
    "    def backward(self, dY): # (classes, 1)\n",
    "        # zero grad\n",
    "        dFC_W = np.zeros_like(self.FC_W)\n",
    "        dfc_b = np.zeros_like(self.fc_b)\n",
    "        \n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dS_next = np.zeros_like(self.S[0])\n",
    "        \n",
    "        dFC_W = dY @ self.O[self.seq_length - 1].T # (classes, 1) @ (1, hidden)\n",
    "        dfc_b = dY # (classes, 1)\n",
    "        dO = self.FC_W.T @ dY\n",
    "        \n",
    "        dV = dO @ self.S[self.seq_length - 1].T\n",
    "        dc = dO\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dS = self.V.T @ dO + dS_next\n",
    "            dA = (1 - self.S[t] ** 2) * dS\n",
    "            dU += dA @ self.X[t].T\n",
    "            dW += dA @ self.S[t - 1].T\n",
    "            db += dA\n",
    "            dS_next = self.W.T @ dA\n",
    "            \n",
    "        return [dU, dW, dV, db, dc, dFC_W, dfc_b]\n",
    "        \n",
    "    def optimizer_step(self, gradients):\n",
    "        for dparam in gradients:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c, self.FC_W, self.fc_b], \n",
    "                                      gradients,\n",
    "                                      [self.mU, self.mW, self.mV, self.mb, self.mc, self.mFC_W, self.mfc_b]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        Y = self.softmax(outputs)\n",
    "        loss = -np.log(Y) * self.one_hot_vector(Y, labels)\n",
    "        return Y, loss\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e)\n",
    "    \n",
    "    def deriv_softmax(self, Y, labels):\n",
    "        dY = np.copy(Y)\n",
    "        for i in range(len(labels)):\n",
    "            dY[labels[i]][i] -= 1\n",
    "        return dY\n",
    "    \n",
    "    def one_hot_vector(self, Y, labels):\n",
    "        out = np.zeros_like(Y)\n",
    "        for i in range(len(labels)):\n",
    "            out[labels[i]][i] = 1\n",
    "        return out\n",
    "    \n",
    "    def predict(self, outputs):\n",
    "        return np.argmax(self.softmax(outputs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09480954]\n",
      " [0.11280908]\n",
      " [0.09361075]\n",
      " [0.10105937]\n",
      " [0.10133655]\n",
      " [0.08675118]\n",
      " [0.11481833]\n",
      " [0.09698714]\n",
      " [0.10430921]\n",
      " [0.09350885]]\n",
      "[6]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.16440416]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 1000/60000 loss 2.3050\n",
      "[[0.09148905]\n",
      " [0.11460342]\n",
      " [0.0959243 ]\n",
      " [0.10118489]\n",
      " [0.09556246]\n",
      " [0.08615557]\n",
      " [0.11056872]\n",
      " [0.11090809]\n",
      " [0.10287707]\n",
      " [0.09072642]]\n",
      "[1]\n",
      "[[0.        ]\n",
      " [2.16627759]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 2000/60000 loss 2.3012\n",
      "[[0.08999564]\n",
      " [0.11700605]\n",
      " [0.09459628]\n",
      " [0.10237167]\n",
      " [0.10150133]\n",
      " [0.09035269]\n",
      " [0.10683918]\n",
      " [0.1058605 ]\n",
      " [0.10203938]\n",
      " [0.08943728]]\n",
      "[1]\n",
      "[[0.       ]\n",
      " [2.1455296]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]\n",
      " [0.       ]]\n",
      "epoch 1/2 iter 3000/60000 loss 2.3016\n",
      "[[0.09044974]\n",
      " [0.11058503]\n",
      " [0.0941043 ]\n",
      " [0.09979343]\n",
      " [0.10204165]\n",
      " [0.0931418 ]\n",
      " [0.10716635]\n",
      " [0.10806227]\n",
      " [0.09959721]\n",
      " [0.09505822]]\n",
      "[4]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.28237423]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 4000/60000 loss 2.3034\n",
      "[[0.10158906]\n",
      " [0.1038154 ]\n",
      " [0.09636947]\n",
      " [0.1000619 ]\n",
      " [0.09957644]\n",
      " [0.094378  ]\n",
      " [0.10225973]\n",
      " [0.10593003]\n",
      " [0.09771084]\n",
      " [0.09830912]]\n",
      "[3]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.30196628]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 5000/60000 loss 2.3064\n",
      "[[0.10769839]\n",
      " [0.10760331]\n",
      " [0.09473475]\n",
      " [0.10174237]\n",
      " [0.1008391 ]\n",
      " [0.09259325]\n",
      " [0.09638819]\n",
      " [0.10194909]\n",
      " [0.09637093]\n",
      " [0.1000806 ]]\n",
      "[7]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.28328168]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 6000/60000 loss 2.3015\n",
      "[[0.10599558]\n",
      " [0.10493298]\n",
      " [0.0944085 ]\n",
      " [0.09849131]\n",
      " [0.09862339]\n",
      " [0.09407776]\n",
      " [0.10009109]\n",
      " [0.10302962]\n",
      " [0.09966813]\n",
      " [0.10068164]]\n",
      "[3]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.31778692]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 7000/60000 loss 2.3042\n",
      "[[0.10494957]\n",
      " [0.11154923]\n",
      " [0.09295618]\n",
      " [0.09821428]\n",
      " [0.09668662]\n",
      " [0.09059649]\n",
      " [0.09664995]\n",
      " [0.1019659 ]\n",
      " [0.102144  ]\n",
      " [0.10428779]]\n",
      "[2]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [2.37562711]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 8000/60000 loss 2.2990\n",
      "[[0.10363023]\n",
      " [0.11371077]\n",
      " [0.0975742 ]\n",
      " [0.10254543]\n",
      " [0.09375759]\n",
      " [0.08779588]\n",
      " [0.09807114]\n",
      " [0.10415222]\n",
      " [0.10007511]\n",
      " [0.09868743]]\n",
      "[9]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.31579772]]\n",
      "epoch 1/2 iter 9000/60000 loss 2.3000\n",
      "[[0.10433799]\n",
      " [0.11038864]\n",
      " [0.09726449]\n",
      " [0.10044214]\n",
      " [0.09558987]\n",
      " [0.0880539 ]\n",
      " [0.09847262]\n",
      " [0.10469417]\n",
      " [0.1007061 ]\n",
      " [0.10005008]]\n",
      "[2]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [2.33032134]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 10000/60000 loss 2.3034\n",
      "[[0.10260583]\n",
      " [0.11122824]\n",
      " [0.0965068 ]\n",
      " [0.09860136]\n",
      " [0.09662144]\n",
      " [0.08691083]\n",
      " [0.09887319]\n",
      " [0.10492821]\n",
      " [0.10307097]\n",
      " [0.10065312]]\n",
      "[7]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.25447885]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 11000/60000 loss 2.3005\n",
      "[[0.10683104]\n",
      " [0.11261009]\n",
      " [0.09832052]\n",
      " [0.09721285]\n",
      " [0.09770386]\n",
      " [0.08384566]\n",
      " [0.09562093]\n",
      " [0.10403443]\n",
      " [0.10108393]\n",
      " [0.10273669]]\n",
      "[4]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.32581424]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 12000/60000 loss 2.2981\n",
      "[[0.10020682]\n",
      " [0.1145621 ]\n",
      " [0.09899743]\n",
      " [0.09785259]\n",
      " [0.10046232]\n",
      " [0.0864499 ]\n",
      " [0.09602904]\n",
      " [0.10374971]\n",
      " [0.10207672]\n",
      " [0.09961336]]\n",
      "[3]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.32429308]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 13000/60000 loss 2.3031\n",
      "[[0.09995306]\n",
      " [0.11496137]\n",
      " [0.09875388]\n",
      " [0.09951502]\n",
      " [0.09829045]\n",
      " [0.08626825]\n",
      " [0.09749298]\n",
      " [0.1049976 ]\n",
      " [0.10033965]\n",
      " [0.09942774]]\n",
      "[3]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.30744673]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 14000/60000 loss 2.3008\n",
      "[[0.09959035]\n",
      " [0.11450875]\n",
      " [0.09815996]\n",
      " [0.10132319]\n",
      " [0.09873034]\n",
      " [0.09059197]\n",
      " [0.09682855]\n",
      " [0.10377917]\n",
      " [0.09827047]\n",
      " [0.09821727]]\n",
      "[9]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.32057325]]\n",
      "epoch 1/2 iter 15000/60000 loss 2.3042\n",
      "[[0.09701814]\n",
      " [0.11264351]\n",
      " [0.09763728]\n",
      " [0.10230828]\n",
      " [0.0993563 ]\n",
      " [0.09126861]\n",
      " [0.10033957]\n",
      " [0.10401922]\n",
      " [0.09675844]\n",
      " [0.09865065]]\n",
      "[9]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.31617047]]\n",
      "epoch 1/2 iter 16000/60000 loss 2.3029\n",
      "[[0.09819097]\n",
      " [0.10892717]\n",
      " [0.09688886]\n",
      " [0.10165736]\n",
      " [0.09771228]\n",
      " [0.09205174]\n",
      " [0.0999193 ]\n",
      " [0.10539405]\n",
      " [0.09876736]\n",
      " [0.10049092]]\n",
      "[7]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [2.25004908]\n",
      " [0.        ]\n",
      " [0.        ]]\n",
      "epoch 1/2 iter 17000/60000 loss 2.3040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-052d2bb485ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderiv_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0miter_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-1a0c65673988>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dY)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mdb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdS_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = My_RNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "iter_loss = 0\n",
    "interval = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        outputs = model.forward(images, hprev)\n",
    "        Y, loss = model.cross_entropy_loss(outputs, labels)\n",
    "        gradients = model.backward(model.deriv_softmax(Y, labels))\n",
    "        model.optimizer_step(gradients)\n",
    "        iter_loss += np.sum(loss)\n",
    "        \n",
    "        if (i + 1) % interval == 0:\n",
    "            print(-np.log(Y))\n",
    "            print(labels)\n",
    "            print(loss)\n",
    "            print(\"epoch {}/{} iter {}/{} loss {:.4f}\".format(epoch + 1, num_epochs, i + 1, total_step, iter_loss / interval))\n",
    "            iter_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 88.68 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    outputs = model.forward(images, hprev)\n",
    "    pred = model.predict(outputs)\n",
    "    total += labels.shape[0]\n",
    "    correct += (pred == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "seq_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "data_path = \"/media/english/ESD-ISO/script_test/Data/mnist/\"\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(c1, c2, w=1,h=1,fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "class My_LSTM(object):\n",
    "    def __init__(self, x_size, hidden_size, num_classes):\n",
    "        self.lr = learning_rate\n",
    "        self.seq_length = seq_length\n",
    "        self.input_size = x_size + hidden_size\n",
    "        \n",
    "        self.W_f = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_g = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_g = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_fc = xavier_init(num_classes, hidden_size, fc=True)\n",
    "        self.b_fc = np.zeros((num_classes, 1))\n",
    "        \n",
    "        self.mW_f = np.zeros_like(self.W_f)\n",
    "        self.mb_f = np.zeros_like(self.b_f)\n",
    "        \n",
    "        self.mW_i = np.zeros_like(self.W_i)\n",
    "        self.mb_i = np.zeros_like(self.b_i)\n",
    "        \n",
    "        self.mW_g = np.zeros_like(self.W_g)\n",
    "        self.mb_g = np.zeros_like(self.b_g)\n",
    "        \n",
    "        self.mW_o = np.zeros_like(self.W_o)\n",
    "        self.mb_o = np.zeros_like(self.b_o)\n",
    "        \n",
    "        self.mW_fc = np.zeros_like(self.W_fc)\n",
    "        self.mb_fc = np.zeros_like(self.b_fc)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.F = {}\n",
    "        self.F_A = {}\n",
    "        \n",
    "        self.I = {}\n",
    "        self.I_A = {}\n",
    "        \n",
    "        self.G = {}\n",
    "        self.G_A = {}\n",
    "        \n",
    "        self.O = {}\n",
    "        self.O_A = {}\n",
    "        \n",
    "        self.C = {}\n",
    "        self.C_A = {}\n",
    "        self.H = {}\n",
    "        \n",
    "    def forward(self, x, hprev, cprev):\n",
    "        self.X = {}\n",
    "        self.F = {}\n",
    "        self.F_A = {}\n",
    "        \n",
    "        self.I = {}\n",
    "        self.I_A = {}\n",
    "        \n",
    "        self.G = {}\n",
    "        self.G_A = {}\n",
    "        \n",
    "        self.O = {}\n",
    "        self.O_A = {}\n",
    "        \n",
    "        self.C = {}\n",
    "        self.C_A = {}\n",
    "        self.H = {}\n",
    "        \n",
    "        self.H[-1] = np.copy(hprev)\n",
    "        self.C[-1] = np.copy(cprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = np.concatenate((self.H[t-1], x[t].T), axis = 0)\n",
    "            \n",
    "            self.F[t] = self.W_f @ self.X[t] + self.b_f\n",
    "            self.F_A[t] = self.sigmoid(self.F[t])\n",
    "            \n",
    "            self.I[t] = self.W_i @ self.X[t] + self.b_i\n",
    "            self.I_A[t] = self.sigmoid(self.I[t])\n",
    "            \n",
    "            self.G[t] = self.W_g @ self.X[t] + self.b_g\n",
    "            self.G_A[t] = np.tanh(self.G[t])\n",
    "            \n",
    "            self.C[t] = self.F_A[t] * self.C[t - 1] + self.I_A[t] * self.G_A[t]\n",
    "            self.C_A[t] = np.tanh(self.C[t])\n",
    "            \n",
    "            self.O[t] = self.W_o @ self.X[t] + self.b_o\n",
    "            self.O_A[t] = self.sigmoid(self.O[t])\n",
    "            \n",
    "            self.H[t] = self.O_A[t] * self.C_A[t]\n",
    "            \n",
    "        output = self.W_fc @ self.H[self.seq_length - 1] + self.b_fc\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        dW_f, db_f = np.zeros_like(self.W_f), np.zeros_like(self.b_f)\n",
    "        dW_i, db_i = np.zeros_like(self.W_i), np.zeros_like(self.b_i)\n",
    "        dW_g, db_g = np.zeros_like(self.W_g), np.zeros_like(self.b_g)\n",
    "        dW_o, db_o = np.zeros_like(self.W_o), np.zeros_like(self.b_o)\n",
    "        dW_fc, db_fc = np.zeros_like(self.W_fc), np.zeros_like(self.b_fc)\n",
    "        \n",
    "        dH_next = np.zeros_like(self.H[0])\n",
    "        dC_next = np.zeros_like(self.C[0])\n",
    "        \n",
    "        dW_fc = dY @ self.H[self.seq_length - 1].T\n",
    "        db_fc = dY\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dh = self.W_fc.T @ dY + dH_next\n",
    "            \n",
    "            dO_A = dh * self.C_A[t]\n",
    "            dO = dO_A * (self.O_A[t] * (1 - self.O_A[t]))\n",
    "            dW_o += dO @ self.X[t].T\n",
    "            db_o += dO\n",
    "            \n",
    "            dC_A = self.O_A[t] * dh\n",
    "            dC = dC_A * (1 - self.C_A[t] ** 2) + dC_next\n",
    "            \n",
    "            dF_A = dC * self.C[t - 1]\n",
    "            dI_A = dC * self.G_A[t]\n",
    "            dG_A = self.I_A[t] * dC\n",
    "            dC_next = self.F_A[t] * dC\n",
    "            \n",
    "            dF = dF_A * (self.F_A[t] * (1 - self.F_A[t]))\n",
    "            dW_f += dF @ self.X[t].T\n",
    "            db_f += dF\n",
    "            \n",
    "            dI = dI_A * (self.I_A[t] * (1 - self.I_A[t]))\n",
    "            dW_i += dI @ self.X[t].T\n",
    "            db_i += dI\n",
    "            \n",
    "            dG = dG_A * (1 - self.G_A[t] ** 2)\n",
    "            dW_g += dG @ self.X[t].T\n",
    "            db_g += dG\n",
    "            \n",
    "            dX = self.W_f.T @ dF + self.W_i.T @ dI + self.W_g.T @ dG + self.W_o.T @ dO\n",
    "            dH_next = dX[:hidden_size, :]\n",
    "        \n",
    "        gradients = [dW_f, db_f, dW_i, db_i, dW_g, db_g, dW_o, db_o, dW_fc, db_fc]\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def optimizer_step(self, gradients):\n",
    "        for dparam in gradients:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        for param, dparam, mem in zip(\n",
    "            [self.W_f, self.b_f, self.W_i, self.b_i, self.W_g, self.b_g, self.W_o, self.b_o, self.W_fc, self.b_fc],\n",
    "            gradients,\n",
    "            [self.mW_f, self.mb_f, self.mW_i, self.mb_i, self.mW_g, self.mb_g, self.mW_o, self.mb_o, self.mW_fc, self.mb_fc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        Y = self.softmax(outputs)\n",
    "        loss = -np.log(Y) * self.one_hot_vector(Y, labels)\n",
    "        return Y, loss\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e)\n",
    "    \n",
    "    def deriv_softmax(self, Y, labels):\n",
    "        dY = np.copy(Y)\n",
    "        for i in range(len(labels)):\n",
    "            dY[labels[i]][i] -= 1\n",
    "        return dY\n",
    "    \n",
    "    def one_hot_vector(self, Y, labels):\n",
    "        out = np.zeros_like(Y)\n",
    "        for i in range(len(labels)):\n",
    "            out[labels[i]][i] = 1\n",
    "        return out\n",
    "    \n",
    "    def predict(self, outputs):\n",
    "        return np.argmax(self.softmax(outputs), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = My_LSTM(input_size, hidden_size, num_classes)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "iter_loss = 0\n",
    "interval = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "        \n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        cprev = np.zeros((hidden_size, 1))\n",
    "        outputs = model.forward(images, hprev, cprev)\n",
    "        Y, loss = model.cross_entropy_loss(outputs, labels)\n",
    "        gradients = model.backward(model.deriv_softmax(Y, labels))\n",
    "        model.optimizer_step(gradients)\n",
    "        iter_loss += np.sum(loss)\n",
    "        if(i + 1) % interval == 0:\n",
    "            print(\"epoch {}/{} iter {}/{} loss {:.4f}\".format(epoch + 1, num_epochs, i + 1, total_step, iter_loss / interval))\n",
    "            iter_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 95.73 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    cprev = np.zeros((hidden_size, 1))\n",
    "    outputs = model.forward(images, hprev, cprev)\n",
    "    pred = model.predict(outputs)\n",
    "    total += labels.shape[0]\n",
    "    correct += (pred == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size,1))\n",
    "        self.c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x - np.max(x))\n",
    "        return p / np.sum(p)\n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros(self.vocab_size, 1)\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t - 1]) + self.b)\n",
    "            os[t] = np.dot(self.V, hs[t]) + self.c\n",
    "            ycap[t] = self.softmax(os[t])\n",
    "        return xs, hs, ycap\n",
    "    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dy\n",
    "            dh = np.dot(self.V.T, dy) + dhnext\n",
    "            dhprec = (1 - hs[t] * hs[t]) * dh\n",
    "            dU += np.dot(dhprec, xs[t].T)\n",
    "            dW += np.dot(dhprec, hs[t-1].T)\n",
    "            db += dhprec\n",
    "            dhnext = np.dot(self.W.T, dhprec)\n",
    "            \n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                     [dU, dW, dV, db, dc],\n",
    "                                     [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    def train(self, data_reader):\n",
    "        iter_num= 0\n",
    "        threshold = 0.1\n",
    "        smooth_loss = -np.log(1.0 / data_reader.vocab_size) * self.seq_length\n",
    "        while (smooth_loss > threshold):\n",
    "            if data_reader.just_started():\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            xs, hs, ps = self.forward(inputs,hprev)\n",
    "            dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "            loss = self.loss(ps, targets)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "def generate_dataset(data_size, length, split_ratio):\n",
    "    X = np.random.uniform(0, 1, (data_size, length, 1))\n",
    "    Y = np.zeros((data_size, length, 1))\n",
    "    threshold = length / 2.\n",
    "    for i in range(data_size):\n",
    "        prefix_sum = 0\n",
    "        for j in range(length):\n",
    "            prefix_sum += X[i][j][0]\n",
    "            Y[i][j][0] = int(prefix_sum > threshold)\n",
    "    split_point = int(data_size * split_ratio)\n",
    "    train_x, test_x = X[:split_point], X[split_point:]\n",
    "    train_y, test_y = Y[:split_point], Y[split_point:]\n",
    "    return np.swapaxes(train_x, 0, 1), np.swapaxes(test_x, 0, 1), \\\n",
    "           np.swapaxes(train_y, 0, 1), np.swapaxes(test_y, 0, 1)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seq_length, lr=0.002):\n",
    "        self.lr = lr\n",
    "        self.seq_length = seq_length\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.U = xavier_init(hidden_dim, input_dim, fc=True)\n",
    "        self.W = xavier_init(hidden_dim, hidden_dim, fc=True)\n",
    "        self.V = xavier_init(output_dim, hidden_dim, fc=True)\n",
    "        self.b = np.zeros((hidden_dim, 1))\n",
    "        self.c = np.zeros((output_dim, 1))\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.H = {}\n",
    "        self.O = {}\n",
    "        self.Y = {}\n",
    "\n",
    "    def forward(self, x, hprev):\n",
    "        self.X = {}\n",
    "        self.H = {}\n",
    "        self.O = {}\n",
    "        self.Y = []\n",
    "        self.H[-1] = np.copy(hprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = x[t:t+1]\n",
    "            self.H[t] = np.tanh(self.U @ self.X[t] + self.W @ self.H[t - 1] + self.b) \n",
    "            self.O[t] = self.V @ self.H[t] + self.c\n",
    "            self.Y.append(self.sigmoid(self.O[t]))\n",
    "        self.Y = np.array(self.Y)\n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, label):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dH_next = np.zeros_like(self.H[0])\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            gt = label[t:t+1]\n",
    "            dY = self.Y[t] - gt \n",
    "            dO = dY * (self.Y[t] * (1 - self.Y[t])) \n",
    "            dV += dO @ self.H[t].T \n",
    "            dc += dO \n",
    "            dH = self.V.T @ dO + dH_next \n",
    "            dTan = (1 - self.H[t] ** 2) * dH \n",
    "            dU += dTan @ self.X[t].T\n",
    "            dW += dTan @ self.H[t - 1].T\n",
    "            db += dTan\n",
    "            dH_next = self.W.T @ dTan\n",
    "        \n",
    "        self.update_model(dU, dW, dV, db, dc)\n",
    "        \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                     [dU, dW, dV, db, dc],\n",
    "                                     [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "def main():\n",
    "    length = 12\n",
    "    data_size = 1000\n",
    "    split_ratio = 0.8\n",
    "    hidden_size = 8\n",
    "    epochs = 500\n",
    "    train_x, test_x, train_y, test_y = generate_dataset(data_size, length, split_ratio)\n",
    "    rnn = RNN(1, hidden_size, 1, length)\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for iters in range(train_x.shape[1]):\n",
    "            label = train_y[:, iters, :]\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            output = rnn.forward(train_x[:, iters, :], hprev)\n",
    "            output = output[:,:,0]\n",
    "            rnn.backward(label)\n",
    "            loss += (output - label) ** 2\n",
    "        print(\"train epoch : {}/{} loss : {:6f}\".format(epoch, epochs, np.sum(loss) / train_x.shape[1]))\n",
    "        \n",
    "        if epoch == 0 or epoch % 100 != 0:\n",
    "            continue\n",
    "        TP_FP = 0\n",
    "        TP_FN = 0\n",
    "        TP = 0\n",
    "        for iters in range(test_x.shape[1]):\n",
    "            label = test_y[:, iters, :]\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            output = rnn.forward(test_x[:, iters, :], hprev)\n",
    "            output = output[:,:,0]\n",
    "            predict = output > 0.5\n",
    "            TP_FP += np.sum(predict)\n",
    "            TP_FN += np.sum(label)\n",
    "            TP += np.sum(np.logical_and(predict, label))\n",
    "            \n",
    "        precision = 0 if TP_FP == 0 else TP / TP_FP\n",
    "        recall = 0 if TP_FN == 0 else TP / TP_FN\n",
    "        print(\"eval epoch : {}/{} precision : {:6f}, recall : {:6f}\".format(epoch, epochs, precision, recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "598a34f14c0623b1f7a078a22e92ee41cc78ba832de8ea969da58fe2ce83dc76"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('MyTorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
