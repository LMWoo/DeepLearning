{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "import third_party_cpp as test_cpp\n",
    "\n",
    "x = torch.randn(4)\n",
    "y = torch.randn(4)\n",
    "print(test_cpp.sigmoid_add(x, y))\n",
    "\n",
    "shape = test_cpp.Shape(5, 3)\n",
    "cArray = test_cpp.NdArray(shape)\n",
    "ret = cArray.ones()\n",
    "ret"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([1.0253, 0.4752, 1.3574, 1.6385])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size,1))\n",
    "        self.c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x - np.max(x))\n",
    "        return p / np.sum(p)\n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros(self.vocab_size, 1)\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t - 1]) + self.b)\n",
    "            os[t] = np.dot(self.V, hs[t]) + self.c\n",
    "            ycap[t] = self.softmax(os[t])\n",
    "        return xs, hs, ycap\n",
    "    \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dy\n",
    "            dh = np.dot(self.V.T, dy) + dhnext\n",
    "            dhprec = (1 - hs[t] * hs[t]) * dh\n",
    "            dU += np.dot(dhprec, xs[t].T)\n",
    "            dW += np.dot(dhprec, hs[t-1].T)\n",
    "            db += dhprec\n",
    "            dhnext = np.dot(self.W.T, dhprec)\n",
    "            \n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                     [dU, dW, dV, db, dc],\n",
    "                                     [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "    \n",
    "    def train(self, data_reader):\n",
    "        iter_num= 0\n",
    "        threshold = 0.1\n",
    "        smooth_loss = -np.log(1.0 / data_reader.vocab_size) * self.seq_length\n",
    "        while (smooth_loss > threshold):\n",
    "            if data_reader.just_started():\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            xs, hs, ps = self.forward(inputs,hprev)\n",
    "            dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "            loss = self.loss(ps, targets)\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "def generate_dataset(data_size, length, split_ratio):\n",
    "    X = np.random.uniform(0, 1, (data_size, length, 1))\n",
    "    Y = np.zeros((data_size, length, 1))\n",
    "    threshold = length / 2.\n",
    "    for i in range(data_size):\n",
    "        prefix_sum = 0\n",
    "        for j in range(length):\n",
    "            prefix_sum += X[i][j][0]\n",
    "            Y[i][j][0] = int(prefix_sum > threshold)\n",
    "    split_point = int(data_size * split_ratio)\n",
    "    train_x, test_x = X[:split_point], X[split_point:]\n",
    "    train_y, test_y = Y[:split_point], Y[split_point:]\n",
    "    return np.swapaxes(train_x, 0, 1), np.swapaxes(test_x, 0, 1), \\\n",
    "           np.swapaxes(train_y, 0, 1), np.swapaxes(test_y, 0, 1)\n",
    "\n",
    "class RNN(object):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, seq_length, lr=0.002):\n",
    "        self.lr = lr\n",
    "        self.seq_length = seq_length\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.U = xavier_init(hidden_dim, input_dim, fc=True)\n",
    "        self.W = xavier_init(hidden_dim, hidden_dim, fc=True)\n",
    "        self.V = xavier_init(output_dim, hidden_dim, fc=True)\n",
    "        self.b = np.zeros((hidden_dim, 1))\n",
    "        self.c = np.zeros((output_dim, 1))\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.H = {}\n",
    "        self.O = {}\n",
    "        self.Y = {}\n",
    "\n",
    "    def forward(self, x, hprev):\n",
    "        self.X = {}\n",
    "        self.H = {}\n",
    "        self.O = {}\n",
    "        self.Y = []\n",
    "        self.H[-1] = np.copy(hprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = x[t:t+1]\n",
    "            self.H[t] = np.tanh(self.U @ self.X[t] + self.W @ self.H[t - 1] + self.b) \n",
    "            self.O[t] = self.V @ self.H[t] + self.c\n",
    "            self.Y.append(self.sigmoid(self.O[t]))\n",
    "        self.Y = np.array(self.Y)\n",
    "        return self.Y\n",
    "    \n",
    "    def backward(self, label):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dH_next = np.zeros_like(self.H[0])\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            gt = label[t:t+1]\n",
    "            dY = self.Y[t] - gt \n",
    "            dO = dY * (self.Y[t] * (1 - self.Y[t])) \n",
    "            dV += dO @ self.H[t].T \n",
    "            dc += dO \n",
    "            dH = self.V.T @ dO + dH_next \n",
    "            dTan = (1 - self.H[t] ** 2) * dH \n",
    "            dU += dTan @ self.X[t].T\n",
    "            dW += dTan @ self.H[t - 1].T\n",
    "            db += dTan\n",
    "            dH_next = self.W.T @ dTan\n",
    "        \n",
    "        self.update_model(dU, dW, dV, db, dc)\n",
    "        \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                     [dU, dW, dV, db, dc],\n",
    "                                     [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "    def sigmoid(self, x):\n",
    "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "    \n",
    "def main():\n",
    "    length = 12\n",
    "    data_size = 1000\n",
    "    split_ratio = 0.8\n",
    "    hidden_size = 8\n",
    "    epochs = 500\n",
    "    train_x, test_x, train_y, test_y = generate_dataset(data_size, length, split_ratio)\n",
    "    rnn = RNN(1, hidden_size, 1, length)\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for iters in range(train_x.shape[1]):\n",
    "            label = train_y[:, iters, :]\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            output = rnn.forward(train_x[:, iters, :], hprev)\n",
    "            output = output[:,:,0]\n",
    "            rnn.backward(label)\n",
    "            loss += (output - label) ** 2\n",
    "        print(\"train epoch : {}/{} loss : {:6f}\".format(epoch, epochs, np.sum(loss) / train_x.shape[1]))\n",
    "        \n",
    "        if epoch == 0 or epoch % 100 != 0:\n",
    "            continue\n",
    "        TP_FP = 0\n",
    "        TP_FN = 0\n",
    "        TP = 0\n",
    "        for iters in range(test_x.shape[1]):\n",
    "            label = test_y[:, iters, :]\n",
    "            hprev = np.zeros((hidden_size, 1))\n",
    "            output = rnn.forward(test_x[:, iters, :], hprev)\n",
    "            output = output[:,:,0]\n",
    "            predict = output > 0.5\n",
    "            TP_FP += np.sum(predict)\n",
    "            TP_FN += np.sum(label)\n",
    "            TP += np.sum(np.logical_and(predict, label))\n",
    "            \n",
    "        precision = 0 if TP_FP == 0 else TP / TP_FP\n",
    "        recall = 0 if TP_FN == 0 else TP / TP_FN\n",
    "        print(\"eval epoch : {}/{} precision : {:6f}, recall : {:6f}\".format(epoch, epochs, precision, recall))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train epoch : 0/500 loss : 1.953975\n",
      "train epoch : 1/500 loss : 1.255561\n",
      "train epoch : 2/500 loss : 1.103833\n",
      "train epoch : 3/500 loss : 1.051097\n",
      "train epoch : 4/500 loss : 1.025201\n",
      "train epoch : 5/500 loss : 1.009782\n",
      "train epoch : 6/500 loss : 0.999488\n",
      "train epoch : 7/500 loss : 0.992081\n",
      "train epoch : 8/500 loss : 0.986461\n",
      "train epoch : 9/500 loss : 0.982027\n",
      "train epoch : 10/500 loss : 0.978419\n",
      "train epoch : 11/500 loss : 0.975410\n",
      "train epoch : 12/500 loss : 0.972851\n",
      "train epoch : 13/500 loss : 0.970637\n",
      "train epoch : 14/500 loss : 0.968695\n",
      "train epoch : 15/500 loss : 0.966971\n",
      "train epoch : 16/500 loss : 0.965424\n",
      "train epoch : 17/500 loss : 0.964023\n",
      "train epoch : 18/500 loss : 0.962745\n",
      "train epoch : 19/500 loss : 0.961569\n",
      "train epoch : 20/500 loss : 0.960482\n",
      "train epoch : 21/500 loss : 0.959471\n",
      "train epoch : 22/500 loss : 0.958526\n",
      "train epoch : 23/500 loss : 0.957638\n",
      "train epoch : 24/500 loss : 0.956802\n",
      "train epoch : 25/500 loss : 0.956010\n",
      "train epoch : 26/500 loss : 0.955259\n",
      "train epoch : 27/500 loss : 0.954544\n",
      "train epoch : 28/500 loss : 0.953861\n",
      "train epoch : 29/500 loss : 0.953208\n",
      "train epoch : 30/500 loss : 0.952581\n",
      "train epoch : 31/500 loss : 0.951979\n",
      "train epoch : 32/500 loss : 0.951400\n",
      "train epoch : 33/500 loss : 0.950841\n",
      "train epoch : 34/500 loss : 0.950301\n",
      "train epoch : 35/500 loss : 0.949779\n",
      "train epoch : 36/500 loss : 0.949274\n",
      "train epoch : 37/500 loss : 0.948783\n",
      "train epoch : 38/500 loss : 0.948308\n",
      "train epoch : 39/500 loss : 0.947845\n",
      "train epoch : 40/500 loss : 0.947396\n",
      "train epoch : 41/500 loss : 0.946958\n",
      "train epoch : 42/500 loss : 0.946531\n",
      "train epoch : 43/500 loss : 0.946115\n",
      "train epoch : 44/500 loss : 0.945709\n",
      "train epoch : 45/500 loss : 0.945313\n",
      "train epoch : 46/500 loss : 0.944926\n",
      "train epoch : 47/500 loss : 0.944547\n",
      "train epoch : 48/500 loss : 0.944177\n",
      "train epoch : 49/500 loss : 0.943814\n",
      "train epoch : 50/500 loss : 0.943459\n",
      "train epoch : 51/500 loss : 0.943111\n",
      "train epoch : 52/500 loss : 0.942770\n",
      "train epoch : 53/500 loss : 0.942436\n",
      "train epoch : 54/500 loss : 0.942108\n",
      "train epoch : 55/500 loss : 0.941787\n",
      "train epoch : 56/500 loss : 0.941471\n",
      "train epoch : 57/500 loss : 0.941161\n",
      "train epoch : 58/500 loss : 0.940856\n",
      "train epoch : 59/500 loss : 0.940556\n",
      "train epoch : 60/500 loss : 0.940262\n",
      "train epoch : 61/500 loss : 0.939973\n",
      "train epoch : 62/500 loss : 0.939688\n",
      "train epoch : 63/500 loss : 0.939408\n",
      "train epoch : 64/500 loss : 0.939132\n",
      "train epoch : 65/500 loss : 0.938861\n",
      "train epoch : 66/500 loss : 0.938594\n",
      "train epoch : 67/500 loss : 0.938331\n",
      "train epoch : 68/500 loss : 0.938071\n",
      "train epoch : 69/500 loss : 0.937816\n",
      "train epoch : 70/500 loss : 0.937564\n",
      "train epoch : 71/500 loss : 0.937316\n",
      "train epoch : 72/500 loss : 0.937072\n",
      "train epoch : 73/500 loss : 0.936830\n",
      "train epoch : 74/500 loss : 0.936593\n",
      "train epoch : 75/500 loss : 0.936358\n",
      "train epoch : 76/500 loss : 0.936126\n",
      "train epoch : 77/500 loss : 0.935898\n",
      "train epoch : 78/500 loss : 0.935672\n",
      "train epoch : 79/500 loss : 0.935449\n",
      "train epoch : 80/500 loss : 0.935229\n",
      "train epoch : 81/500 loss : 0.935012\n",
      "train epoch : 82/500 loss : 0.934797\n",
      "train epoch : 83/500 loss : 0.934585\n",
      "train epoch : 84/500 loss : 0.934376\n",
      "train epoch : 85/500 loss : 0.934169\n",
      "train epoch : 86/500 loss : 0.933964\n",
      "train epoch : 87/500 loss : 0.933762\n",
      "train epoch : 88/500 loss : 0.933562\n",
      "train epoch : 89/500 loss : 0.933364\n",
      "train epoch : 90/500 loss : 0.933168\n",
      "train epoch : 91/500 loss : 0.932975\n",
      "train epoch : 92/500 loss : 0.932783\n",
      "train epoch : 93/500 loss : 0.932594\n",
      "train epoch : 94/500 loss : 0.932406\n",
      "train epoch : 95/500 loss : 0.932221\n",
      "train epoch : 96/500 loss : 0.932037\n",
      "train epoch : 97/500 loss : 0.931855\n",
      "train epoch : 98/500 loss : 0.931675\n",
      "train epoch : 99/500 loss : 0.931497\n",
      "train epoch : 100/500 loss : 0.931320\n",
      "eval epoch : 100/500 precision : 0.000000, recall : 0.000000\n",
      "train epoch : 101/500 loss : 0.931145\n",
      "train epoch : 102/500 loss : 0.930972\n",
      "train epoch : 103/500 loss : 0.930800\n",
      "train epoch : 104/500 loss : 0.930630\n",
      "train epoch : 105/500 loss : 0.930461\n",
      "train epoch : 106/500 loss : 0.930294\n",
      "train epoch : 107/500 loss : 0.930128\n",
      "train epoch : 108/500 loss : 0.929964\n",
      "train epoch : 109/500 loss : 0.929801\n",
      "train epoch : 110/500 loss : 0.929639\n",
      "train epoch : 111/500 loss : 0.929479\n",
      "train epoch : 112/500 loss : 0.929319\n",
      "train epoch : 113/500 loss : 0.929162\n",
      "train epoch : 114/500 loss : 0.929005\n",
      "train epoch : 115/500 loss : 0.928850\n",
      "train epoch : 116/500 loss : 0.928695\n",
      "train epoch : 117/500 loss : 0.928542\n",
      "train epoch : 118/500 loss : 0.928390\n",
      "train epoch : 119/500 loss : 0.928239\n",
      "train epoch : 120/500 loss : 0.928090\n",
      "train epoch : 121/500 loss : 0.927941\n",
      "train epoch : 122/500 loss : 0.927793\n",
      "train epoch : 123/500 loss : 0.927646\n",
      "train epoch : 124/500 loss : 0.927500\n",
      "train epoch : 125/500 loss : 0.927355\n",
      "train epoch : 126/500 loss : 0.927211\n",
      "train epoch : 127/500 loss : 0.927068\n",
      "train epoch : 128/500 loss : 0.926926\n",
      "train epoch : 129/500 loss : 0.926785\n",
      "train epoch : 130/500 loss : 0.926644\n",
      "train epoch : 131/500 loss : 0.926504\n",
      "train epoch : 132/500 loss : 0.926365\n",
      "train epoch : 133/500 loss : 0.926227\n",
      "train epoch : 134/500 loss : 0.926090\n",
      "train epoch : 135/500 loss : 0.925953\n",
      "train epoch : 136/500 loss : 0.925817\n",
      "train epoch : 137/500 loss : 0.925681\n",
      "train epoch : 138/500 loss : 0.925547\n",
      "train epoch : 139/500 loss : 0.925412\n",
      "train epoch : 140/500 loss : 0.925279\n",
      "train epoch : 141/500 loss : 0.925146\n",
      "train epoch : 142/500 loss : 0.925014\n",
      "train epoch : 143/500 loss : 0.924882\n",
      "train epoch : 144/500 loss : 0.924751\n",
      "train epoch : 145/500 loss : 0.924620\n",
      "train epoch : 146/500 loss : 0.924490\n",
      "train epoch : 147/500 loss : 0.924360\n",
      "train epoch : 148/500 loss : 0.924231\n",
      "train epoch : 149/500 loss : 0.924102\n",
      "train epoch : 150/500 loss : 0.923974\n",
      "train epoch : 151/500 loss : 0.923846\n",
      "train epoch : 152/500 loss : 0.923718\n",
      "train epoch : 153/500 loss : 0.923591\n",
      "train epoch : 154/500 loss : 0.923464\n",
      "train epoch : 155/500 loss : 0.923338\n",
      "train epoch : 156/500 loss : 0.923211\n",
      "train epoch : 157/500 loss : 0.923086\n",
      "train epoch : 158/500 loss : 0.922960\n",
      "train epoch : 159/500 loss : 0.922835\n",
      "train epoch : 160/500 loss : 0.922710\n",
      "train epoch : 161/500 loss : 0.922585\n",
      "train epoch : 162/500 loss : 0.922460\n",
      "train epoch : 163/500 loss : 0.922336\n",
      "train epoch : 164/500 loss : 0.922211\n",
      "train epoch : 165/500 loss : 0.922087\n",
      "train epoch : 166/500 loss : 0.921963\n",
      "train epoch : 167/500 loss : 0.921839\n",
      "train epoch : 168/500 loss : 0.921715\n",
      "train epoch : 169/500 loss : 0.921592\n",
      "train epoch : 170/500 loss : 0.921468\n",
      "train epoch : 171/500 loss : 0.921344\n",
      "train epoch : 172/500 loss : 0.921221\n",
      "train epoch : 173/500 loss : 0.921097\n",
      "train epoch : 174/500 loss : 0.920973\n",
      "train epoch : 175/500 loss : 0.920850\n",
      "train epoch : 176/500 loss : 0.920726\n",
      "train epoch : 177/500 loss : 0.920602\n",
      "train epoch : 178/500 loss : 0.920478\n",
      "train epoch : 179/500 loss : 0.920354\n",
      "train epoch : 180/500 loss : 0.920230\n",
      "train epoch : 181/500 loss : 0.920105\n",
      "train epoch : 182/500 loss : 0.919980\n",
      "train epoch : 183/500 loss : 0.919855\n",
      "train epoch : 184/500 loss : 0.919730\n",
      "train epoch : 185/500 loss : 0.919605\n",
      "train epoch : 186/500 loss : 0.919479\n",
      "train epoch : 187/500 loss : 0.919353\n",
      "train epoch : 188/500 loss : 0.919226\n",
      "train epoch : 189/500 loss : 0.919100\n",
      "train epoch : 190/500 loss : 0.918972\n",
      "train epoch : 191/500 loss : 0.918844\n",
      "train epoch : 192/500 loss : 0.918716\n",
      "train epoch : 193/500 loss : 0.918588\n",
      "train epoch : 194/500 loss : 0.918458\n",
      "train epoch : 195/500 loss : 0.918328\n",
      "train epoch : 196/500 loss : 0.918198\n",
      "train epoch : 197/500 loss : 0.918067\n",
      "train epoch : 198/500 loss : 0.917935\n",
      "train epoch : 199/500 loss : 0.917802\n",
      "train epoch : 200/500 loss : 0.917669\n",
      "eval epoch : 200/500 precision : 0.000000, recall : 0.000000\n",
      "train epoch : 201/500 loss : 0.917535\n",
      "train epoch : 202/500 loss : 0.917400\n",
      "train epoch : 203/500 loss : 0.917264\n",
      "train epoch : 204/500 loss : 0.917127\n",
      "train epoch : 205/500 loss : 0.916990\n",
      "train epoch : 206/500 loss : 0.916851\n",
      "train epoch : 207/500 loss : 0.916711\n",
      "train epoch : 208/500 loss : 0.916570\n",
      "train epoch : 209/500 loss : 0.916428\n",
      "train epoch : 210/500 loss : 0.916284\n",
      "train epoch : 211/500 loss : 0.916140\n",
      "train epoch : 212/500 loss : 0.915993\n",
      "train epoch : 213/500 loss : 0.915846\n",
      "train epoch : 214/500 loss : 0.915697\n",
      "train epoch : 215/500 loss : 0.915546\n",
      "train epoch : 216/500 loss : 0.915394\n",
      "train epoch : 217/500 loss : 0.915240\n",
      "train epoch : 218/500 loss : 0.915084\n",
      "train epoch : 219/500 loss : 0.914926\n",
      "train epoch : 220/500 loss : 0.914766\n",
      "train epoch : 221/500 loss : 0.914604\n",
      "train epoch : 222/500 loss : 0.914440\n",
      "train epoch : 223/500 loss : 0.914274\n",
      "train epoch : 224/500 loss : 0.914105\n",
      "train epoch : 225/500 loss : 0.913934\n",
      "train epoch : 226/500 loss : 0.913759\n",
      "train epoch : 227/500 loss : 0.913582\n",
      "train epoch : 228/500 loss : 0.913403\n",
      "train epoch : 229/500 loss : 0.913220\n",
      "train epoch : 230/500 loss : 0.913033\n",
      "train epoch : 231/500 loss : 0.912843\n",
      "train epoch : 232/500 loss : 0.912650\n",
      "train epoch : 233/500 loss : 0.912453\n",
      "train epoch : 234/500 loss : 0.912251\n",
      "train epoch : 235/500 loss : 0.912045\n",
      "train epoch : 236/500 loss : 0.911835\n",
      "train epoch : 237/500 loss : 0.911620\n",
      "train epoch : 238/500 loss : 0.911400\n",
      "train epoch : 239/500 loss : 0.911174\n",
      "train epoch : 240/500 loss : 0.910942\n",
      "train epoch : 241/500 loss : 0.910705\n",
      "train epoch : 242/500 loss : 0.910460\n",
      "train epoch : 243/500 loss : 0.910209\n",
      "train epoch : 244/500 loss : 0.909950\n",
      "train epoch : 245/500 loss : 0.909683\n",
      "train epoch : 246/500 loss : 0.909408\n",
      "train epoch : 247/500 loss : 0.909124\n",
      "train epoch : 248/500 loss : 0.908829\n",
      "train epoch : 249/500 loss : 0.908524\n",
      "train epoch : 250/500 loss : 0.908207\n",
      "train epoch : 251/500 loss : 0.907878\n",
      "train epoch : 252/500 loss : 0.907536\n",
      "train epoch : 253/500 loss : 0.907179\n",
      "train epoch : 254/500 loss : 0.906805\n",
      "train epoch : 255/500 loss : 0.906415\n",
      "train epoch : 256/500 loss : 0.906005\n",
      "train epoch : 257/500 loss : 0.905575\n",
      "train epoch : 258/500 loss : 0.905121\n",
      "train epoch : 259/500 loss : 0.904642\n",
      "train epoch : 260/500 loss : 0.904134\n",
      "train epoch : 261/500 loss : 0.903595\n",
      "train epoch : 262/500 loss : 0.903021\n",
      "train epoch : 263/500 loss : 0.902407\n",
      "train epoch : 264/500 loss : 0.901749\n",
      "train epoch : 265/500 loss : 0.901040\n",
      "train epoch : 266/500 loss : 0.900273\n",
      "train epoch : 267/500 loss : 0.899441\n",
      "train epoch : 268/500 loss : 0.898534\n",
      "train epoch : 269/500 loss : 0.897539\n",
      "train epoch : 270/500 loss : 0.896444\n",
      "train epoch : 271/500 loss : 0.895231\n",
      "train epoch : 272/500 loss : 0.893882\n",
      "train epoch : 273/500 loss : 0.892372\n",
      "train epoch : 274/500 loss : 0.890674\n",
      "train epoch : 275/500 loss : 0.888758\n",
      "train epoch : 276/500 loss : 0.886587\n",
      "train epoch : 277/500 loss : 0.884124\n",
      "train epoch : 278/500 loss : 0.881327\n",
      "train epoch : 279/500 loss : 0.878165\n",
      "train epoch : 280/500 loss : 0.874619\n",
      "train epoch : 281/500 loss : 0.870694\n",
      "train epoch : 282/500 loss : 0.866430\n",
      "train epoch : 283/500 loss : 0.861899\n",
      "train epoch : 284/500 loss : 0.857205\n",
      "train epoch : 285/500 loss : 0.852459\n",
      "train epoch : 286/500 loss : 0.847755\n",
      "train epoch : 287/500 loss : 0.843158\n",
      "train epoch : 288/500 loss : 0.838698\n",
      "train epoch : 289/500 loss : 0.834386\n",
      "train epoch : 290/500 loss : 0.830214\n",
      "train epoch : 291/500 loss : 0.826167\n",
      "train epoch : 292/500 loss : 0.822229\n",
      "train epoch : 293/500 loss : 0.818383\n",
      "train epoch : 294/500 loss : 0.814611\n",
      "train epoch : 295/500 loss : 0.810901\n",
      "train epoch : 296/500 loss : 0.807241\n",
      "train epoch : 297/500 loss : 0.803624\n",
      "train epoch : 298/500 loss : 0.800041\n",
      "train epoch : 299/500 loss : 0.796488\n",
      "train epoch : 300/500 loss : 0.792962\n",
      "eval epoch : 300/500 precision : 0.000000, recall : 0.000000\n",
      "train epoch : 301/500 loss : 0.789459\n",
      "train epoch : 302/500 loss : 0.785978\n",
      "train epoch : 303/500 loss : 0.782519\n",
      "train epoch : 304/500 loss : 0.779081\n",
      "train epoch : 305/500 loss : 0.775663\n",
      "train epoch : 306/500 loss : 0.772268\n",
      "train epoch : 307/500 loss : 0.768894\n",
      "train epoch : 308/500 loss : 0.765542\n",
      "train epoch : 309/500 loss : 0.762215\n",
      "train epoch : 310/500 loss : 0.758912\n",
      "train epoch : 311/500 loss : 0.755634\n",
      "train epoch : 312/500 loss : 0.752382\n",
      "train epoch : 313/500 loss : 0.749157\n",
      "train epoch : 314/500 loss : 0.745960\n",
      "train epoch : 315/500 loss : 0.742791\n",
      "train epoch : 316/500 loss : 0.739651\n",
      "train epoch : 317/500 loss : 0.736541\n",
      "train epoch : 318/500 loss : 0.733460\n",
      "train epoch : 319/500 loss : 0.730410\n",
      "train epoch : 320/500 loss : 0.727391\n",
      "train epoch : 321/500 loss : 0.724403\n",
      "train epoch : 322/500 loss : 0.721446\n",
      "train epoch : 323/500 loss : 0.718520\n",
      "train epoch : 324/500 loss : 0.715626\n",
      "train epoch : 325/500 loss : 0.712763\n",
      "train epoch : 326/500 loss : 0.709932\n",
      "train epoch : 327/500 loss : 0.707132\n",
      "train epoch : 328/500 loss : 0.704363\n",
      "train epoch : 329/500 loss : 0.701626\n",
      "train epoch : 330/500 loss : 0.698920\n",
      "train epoch : 331/500 loss : 0.696244\n",
      "train epoch : 332/500 loss : 0.693600\n",
      "train epoch : 333/500 loss : 0.690986\n",
      "train epoch : 334/500 loss : 0.688402\n",
      "train epoch : 335/500 loss : 0.685849\n",
      "train epoch : 336/500 loss : 0.683325\n",
      "train epoch : 337/500 loss : 0.680831\n",
      "train epoch : 338/500 loss : 0.678366\n",
      "train epoch : 339/500 loss : 0.675930\n",
      "train epoch : 340/500 loss : 0.673523\n",
      "train epoch : 341/500 loss : 0.671144\n",
      "train epoch : 342/500 loss : 0.668794\n",
      "train epoch : 343/500 loss : 0.666471\n",
      "train epoch : 344/500 loss : 0.664176\n",
      "train epoch : 345/500 loss : 0.661908\n",
      "train epoch : 346/500 loss : 0.659666\n",
      "train epoch : 347/500 loss : 0.657452\n",
      "train epoch : 348/500 loss : 0.655264\n",
      "train epoch : 349/500 loss : 0.653101\n",
      "train epoch : 350/500 loss : 0.650965\n",
      "train epoch : 351/500 loss : 0.648854\n",
      "train epoch : 352/500 loss : 0.646767\n",
      "train epoch : 353/500 loss : 0.644706\n",
      "train epoch : 354/500 loss : 0.642669\n",
      "train epoch : 355/500 loss : 0.640656\n",
      "train epoch : 356/500 loss : 0.638668\n",
      "train epoch : 357/500 loss : 0.636703\n",
      "train epoch : 358/500 loss : 0.634761\n",
      "train epoch : 359/500 loss : 0.632842\n",
      "train epoch : 360/500 loss : 0.630946\n",
      "train epoch : 361/500 loss : 0.629073\n",
      "train epoch : 362/500 loss : 0.627221\n",
      "train epoch : 363/500 loss : 0.625392\n",
      "train epoch : 364/500 loss : 0.623584\n",
      "train epoch : 365/500 loss : 0.621797\n",
      "train epoch : 366/500 loss : 0.620031\n",
      "train epoch : 367/500 loss : 0.618286\n",
      "train epoch : 368/500 loss : 0.616561\n",
      "train epoch : 369/500 loss : 0.614857\n",
      "train epoch : 370/500 loss : 0.613172\n",
      "train epoch : 371/500 loss : 0.611508\n",
      "train epoch : 372/500 loss : 0.609862\n",
      "train epoch : 373/500 loss : 0.608236\n",
      "train epoch : 374/500 loss : 0.606628\n",
      "train epoch : 375/500 loss : 0.605040\n",
      "train epoch : 376/500 loss : 0.603469\n",
      "train epoch : 377/500 loss : 0.601917\n",
      "train epoch : 378/500 loss : 0.600383\n",
      "train epoch : 379/500 loss : 0.598867\n",
      "train epoch : 380/500 loss : 0.597368\n",
      "train epoch : 381/500 loss : 0.595887\n",
      "train epoch : 382/500 loss : 0.594422\n",
      "train epoch : 383/500 loss : 0.592975\n",
      "train epoch : 384/500 loss : 0.591544\n",
      "train epoch : 385/500 loss : 0.590129\n",
      "train epoch : 386/500 loss : 0.588730\n",
      "train epoch : 387/500 loss : 0.587347\n",
      "train epoch : 388/500 loss : 0.585980\n",
      "train epoch : 389/500 loss : 0.584628\n",
      "train epoch : 390/500 loss : 0.583291\n",
      "train epoch : 391/500 loss : 0.581969\n",
      "train epoch : 392/500 loss : 0.580661\n",
      "train epoch : 393/500 loss : 0.579369\n",
      "train epoch : 394/500 loss : 0.578090\n",
      "train epoch : 395/500 loss : 0.576826\n",
      "train epoch : 396/500 loss : 0.575575\n",
      "train epoch : 397/500 loss : 0.574339\n",
      "train epoch : 398/500 loss : 0.573115\n",
      "train epoch : 399/500 loss : 0.571905\n",
      "train epoch : 400/500 loss : 0.570709\n",
      "eval epoch : 400/500 precision : 0.777778, recall : 0.114754\n",
      "train epoch : 401/500 loss : 0.569525\n",
      "train epoch : 402/500 loss : 0.568354\n",
      "train epoch : 403/500 loss : 0.567195\n",
      "train epoch : 404/500 loss : 0.566049\n",
      "train epoch : 405/500 loss : 0.564915\n",
      "train epoch : 406/500 loss : 0.563793\n",
      "train epoch : 407/500 loss : 0.562683\n",
      "train epoch : 408/500 loss : 0.561585\n",
      "train epoch : 409/500 loss : 0.560498\n",
      "train epoch : 410/500 loss : 0.559422\n",
      "train epoch : 411/500 loss : 0.558357\n",
      "train epoch : 412/500 loss : 0.557304\n",
      "train epoch : 413/500 loss : 0.556261\n",
      "train epoch : 414/500 loss : 0.555229\n",
      "train epoch : 415/500 loss : 0.554207\n",
      "train epoch : 416/500 loss : 0.553196\n",
      "train epoch : 417/500 loss : 0.552195\n",
      "train epoch : 418/500 loss : 0.551204\n",
      "train epoch : 419/500 loss : 0.550223\n",
      "train epoch : 420/500 loss : 0.549252\n",
      "train epoch : 421/500 loss : 0.548291\n",
      "train epoch : 422/500 loss : 0.547338\n",
      "train epoch : 423/500 loss : 0.546396\n",
      "train epoch : 424/500 loss : 0.545462\n",
      "train epoch : 425/500 loss : 0.544537\n",
      "train epoch : 426/500 loss : 0.543622\n",
      "train epoch : 427/500 loss : 0.542715\n",
      "train epoch : 428/500 loss : 0.541816\n",
      "train epoch : 429/500 loss : 0.540927\n",
      "train epoch : 430/500 loss : 0.540045\n",
      "train epoch : 431/500 loss : 0.539172\n",
      "train epoch : 432/500 loss : 0.538307\n",
      "train epoch : 433/500 loss : 0.537451\n",
      "train epoch : 434/500 loss : 0.536602\n",
      "train epoch : 435/500 loss : 0.535761\n",
      "train epoch : 436/500 loss : 0.534927\n",
      "train epoch : 437/500 loss : 0.534102\n",
      "train epoch : 438/500 loss : 0.533283\n",
      "train epoch : 439/500 loss : 0.532473\n",
      "train epoch : 440/500 loss : 0.531669\n",
      "train epoch : 441/500 loss : 0.530873\n",
      "train epoch : 442/500 loss : 0.530083\n",
      "train epoch : 443/500 loss : 0.529301\n",
      "train epoch : 444/500 loss : 0.528525\n",
      "train epoch : 445/500 loss : 0.527757\n",
      "train epoch : 446/500 loss : 0.526995\n",
      "train epoch : 447/500 loss : 0.526239\n",
      "train epoch : 448/500 loss : 0.525491\n",
      "train epoch : 449/500 loss : 0.524748\n",
      "train epoch : 450/500 loss : 0.524012\n",
      "train epoch : 451/500 loss : 0.523282\n",
      "train epoch : 452/500 loss : 0.522559\n",
      "train epoch : 453/500 loss : 0.521841\n",
      "train epoch : 454/500 loss : 0.521129\n",
      "train epoch : 455/500 loss : 0.520424\n",
      "train epoch : 456/500 loss : 0.519724\n",
      "train epoch : 457/500 loss : 0.519030\n",
      "train epoch : 458/500 loss : 0.518341\n",
      "train epoch : 459/500 loss : 0.517658\n",
      "train epoch : 460/500 loss : 0.516981\n",
      "train epoch : 461/500 loss : 0.516309\n",
      "train epoch : 462/500 loss : 0.515643\n",
      "train epoch : 463/500 loss : 0.514981\n",
      "train epoch : 464/500 loss : 0.514325\n",
      "train epoch : 465/500 loss : 0.513674\n",
      "train epoch : 466/500 loss : 0.513028\n",
      "train epoch : 467/500 loss : 0.512387\n",
      "train epoch : 468/500 loss : 0.511751\n",
      "train epoch : 469/500 loss : 0.511120\n",
      "train epoch : 470/500 loss : 0.510494\n",
      "train epoch : 471/500 loss : 0.509873\n",
      "train epoch : 472/500 loss : 0.509256\n",
      "train epoch : 473/500 loss : 0.508644\n",
      "train epoch : 474/500 loss : 0.508036\n",
      "train epoch : 475/500 loss : 0.507433\n",
      "train epoch : 476/500 loss : 0.506835\n",
      "train epoch : 477/500 loss : 0.506241\n",
      "train epoch : 478/500 loss : 0.505651\n",
      "train epoch : 479/500 loss : 0.505066\n",
      "train epoch : 480/500 loss : 0.504484\n",
      "train epoch : 481/500 loss : 0.503907\n",
      "train epoch : 482/500 loss : 0.503335\n",
      "train epoch : 483/500 loss : 0.502766\n",
      "train epoch : 484/500 loss : 0.502201\n",
      "train epoch : 485/500 loss : 0.501640\n",
      "train epoch : 486/500 loss : 0.501083\n",
      "train epoch : 487/500 loss : 0.500530\n",
      "train epoch : 488/500 loss : 0.499981\n",
      "train epoch : 489/500 loss : 0.499436\n",
      "train epoch : 490/500 loss : 0.498894\n",
      "train epoch : 491/500 loss : 0.498356\n",
      "train epoch : 492/500 loss : 0.497822\n",
      "train epoch : 493/500 loss : 0.497292\n",
      "train epoch : 494/500 loss : 0.496764\n",
      "train epoch : 495/500 loss : 0.496241\n",
      "train epoch : 496/500 loss : 0.495720\n",
      "train epoch : 497/500 loss : 0.495204\n",
      "train epoch : 498/500 loss : 0.494690\n",
      "train epoch : 499/500 loss : 0.494180\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "seq_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "data_path = \"/media/lee/ESD-ISO/script_test/Data/mnist/\"\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def xavier_init(c1, c2, w=1, h=1, fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "class My_RNN(object):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        self.lr = learning_rate\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.U = xavier_init(hidden_size, input_size, fc=True) # rnn input parameters\n",
    "        self.W = xavier_init(hidden_size, hidden_size, fc=True) # rnn hidden parameters\n",
    "        self.V = xavier_init(hidden_size, hidden_size, fc=True) # rnn output parameters\n",
    "        self.b = np.zeros((hidden_size, 1)) # rnn input parameters\n",
    "        self.c = np.zeros((hidden_size, 1)) # rnn output parameters\n",
    "        \n",
    "        self.FC_W = xavier_init(num_classes, hidden_size, fc=True) # fc parameters\n",
    "        self.fc_b = np.zeros((num_classes, 1)) # fc parameters\n",
    "        \n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "        \n",
    "        self.mFC_W = np.zeros_like(self.FC_W)\n",
    "        self.mfc_b = np.zeros_like(self.fc_b)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.A = {}\n",
    "        self.S = {}\n",
    "        self.O = {}\n",
    "        self.FC_O = {}\n",
    "        \n",
    "    def forward(self, x, hprev):\n",
    "        self.S[-1] = np.copy(hprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = x[t].T\n",
    "            self.A[t] = self.U @ self.X[t] + self.W @ self.S[t - 1] + self.b\n",
    "            self.S[t] = np.tanh(self.A[t])\n",
    "            self.O[t] = self.V @ self.S[t] + self.c # (hidden, hidden) @ (hidden, 1) + (hidden, 1)\n",
    "        \n",
    "        self.FC_O = self.FC_W @ self.O[self.seq_length - 1] + self.fc_b # (classes, hidden) @ (hidden, 1) + (classes, 1)\n",
    "        \n",
    "        return self.FC_O # (classes, 1)\n",
    "    \n",
    "    def backward(self, dY): # (classes, 1)\n",
    "        # zero grad\n",
    "        dFC_W = np.zeros_like(self.FC_W)\n",
    "        dfc_b = np.zeros_like(self.fc_b)\n",
    "        \n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dS_next = np.zeros_like(self.S[0])\n",
    "        \n",
    "        dFC_W = dY @ self.O[self.seq_length - 1].T # (classes, 1) @ (1, hidden)\n",
    "        dfc_b = dY # (classes, 1)\n",
    "        dO = self.FC_W.T @ dY\n",
    "        \n",
    "        dV = dO @ self.S[self.seq_length - 1].T\n",
    "        dc = dO\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dS = self.V.T @ dO + dS_next\n",
    "            dA = (1 - self.S[t] ** 2) * dS\n",
    "            dU += dA @ self.X[t].T\n",
    "            dW += dA @ self.S[t - 1].T\n",
    "            db += dA\n",
    "            dS_next = self.W.T @ dA\n",
    "            \n",
    "        return [dU, dW, dV, db, dc, dFC_W, dfc_b]\n",
    "        \n",
    "    def optimizer_step(self, gradients):\n",
    "        for dparam in gradients:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "            \n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c, self.FC_W, self.fc_b], \n",
    "                                      gradients,\n",
    "                                      [self.mU, self.mW, self.mV, self.mb, self.mc, self.mFC_W, self.mfc_b]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        Y = self.softmax(outputs)\n",
    "        loss = -np.log(Y) * self.one_hot_vector(Y, labels)\n",
    "        return Y, loss\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e)\n",
    "    \n",
    "    def deriv_softmax(self, Y, labels):\n",
    "        dY = np.copy(Y)\n",
    "        for i in range(len(labels)):\n",
    "            dY[labels[i]][i] -= 1\n",
    "        return dY\n",
    "    \n",
    "    def one_hot_vector(self, Y, labels):\n",
    "        out = np.zeros_like(Y)\n",
    "        for i in range(len(labels)):\n",
    "            out[labels[i]][i] = 1\n",
    "        return out\n",
    "    \n",
    "    def predict(self, outputs):\n",
    "        return np.argmax(self.softmax(outputs), 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = My_RNN(input_size, hidden_size, num_classes)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "iter_loss = 0\n",
    "interval = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "        \n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        outputs = model.forward(images, hprev)\n",
    "        Y, loss = model.cross_entropy_loss(outputs, labels)\n",
    "        gradients = model.backward(model.deriv_softmax(Y, labels))\n",
    "        model.optimizer_step(gradients)\n",
    "        iter_loss += np.sum(loss)\n",
    "        if (i + 1) % interval == 0:\n",
    "            print(\"epoch {}/{} iter {}/{} loss {:.4f}\".format(epoch + 1, num_epochs, i + 1, total_step, iter_loss / interval))\n",
    "            iter_loss = 0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1/2 iter 1000/60000 loss 2.2714\n",
      "epoch 1/2 iter 2000/60000 loss 1.8728\n",
      "epoch 1/2 iter 3000/60000 loss 1.5624\n",
      "epoch 1/2 iter 4000/60000 loss 1.2485\n",
      "epoch 1/2 iter 5000/60000 loss 1.1432\n",
      "epoch 1/2 iter 6000/60000 loss 1.0680\n",
      "epoch 1/2 iter 7000/60000 loss 0.9373\n",
      "epoch 1/2 iter 8000/60000 loss 0.9352\n",
      "epoch 1/2 iter 9000/60000 loss 0.8049\n",
      "epoch 1/2 iter 10000/60000 loss 0.8140\n",
      "epoch 1/2 iter 11000/60000 loss 0.7530\n",
      "epoch 1/2 iter 12000/60000 loss 0.8065\n",
      "epoch 1/2 iter 13000/60000 loss 0.8062\n",
      "epoch 1/2 iter 14000/60000 loss 0.6526\n",
      "epoch 1/2 iter 15000/60000 loss 0.6879\n",
      "epoch 1/2 iter 16000/60000 loss 0.6408\n",
      "epoch 1/2 iter 17000/60000 loss 0.6369\n",
      "epoch 1/2 iter 18000/60000 loss 0.6346\n",
      "epoch 1/2 iter 19000/60000 loss 0.5935\n",
      "epoch 1/2 iter 20000/60000 loss 0.5328\n",
      "epoch 1/2 iter 21000/60000 loss 0.5646\n",
      "epoch 1/2 iter 22000/60000 loss 0.5039\n",
      "epoch 1/2 iter 23000/60000 loss 0.4769\n",
      "epoch 1/2 iter 24000/60000 loss 0.5605\n",
      "epoch 1/2 iter 25000/60000 loss 0.4959\n",
      "epoch 1/2 iter 26000/60000 loss 0.5048\n",
      "epoch 1/2 iter 27000/60000 loss 0.5079\n",
      "epoch 1/2 iter 28000/60000 loss 0.4571\n",
      "epoch 1/2 iter 29000/60000 loss 0.5084\n",
      "epoch 1/2 iter 30000/60000 loss 0.4846\n",
      "epoch 1/2 iter 31000/60000 loss 0.5073\n",
      "epoch 1/2 iter 32000/60000 loss 0.4890\n",
      "epoch 1/2 iter 33000/60000 loss 0.4382\n",
      "epoch 1/2 iter 34000/60000 loss 0.4943\n",
      "epoch 1/2 iter 35000/60000 loss 0.4775\n",
      "epoch 1/2 iter 36000/60000 loss 0.4568\n",
      "epoch 1/2 iter 37000/60000 loss 0.4507\n",
      "epoch 1/2 iter 38000/60000 loss 0.4620\n",
      "epoch 1/2 iter 39000/60000 loss 0.4160\n",
      "epoch 1/2 iter 40000/60000 loss 0.4289\n",
      "epoch 1/2 iter 41000/60000 loss 0.3790\n",
      "epoch 1/2 iter 42000/60000 loss 0.3749\n",
      "epoch 1/2 iter 43000/60000 loss 0.3620\n",
      "epoch 1/2 iter 44000/60000 loss 0.4288\n",
      "epoch 1/2 iter 45000/60000 loss 0.3485\n",
      "epoch 1/2 iter 46000/60000 loss 0.4053\n",
      "epoch 1/2 iter 47000/60000 loss 0.3841\n",
      "epoch 1/2 iter 48000/60000 loss 0.4183\n",
      "epoch 1/2 iter 49000/60000 loss 0.4342\n",
      "epoch 1/2 iter 50000/60000 loss 0.4208\n",
      "epoch 1/2 iter 51000/60000 loss 0.4178\n",
      "epoch 1/2 iter 52000/60000 loss 0.4352\n",
      "epoch 1/2 iter 53000/60000 loss 0.4522\n",
      "epoch 1/2 iter 54000/60000 loss 0.3683\n",
      "epoch 1/2 iter 55000/60000 loss 0.3639\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f3fdcdd4ea8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderiv_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0miter_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-bc716fca7204>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dY)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mdb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mdS_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdFC_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfc_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    outputs = model.forward(images, hprev)\n",
    "    pred = model.predict(outputs)\n",
    "    total += labels.shape[0]\n",
    "    correct += (pred == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 60.29 %\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "seq_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "batch_size = 1\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "data_path = \"/media/english/ESD-ISO/script_test/Data/mnist/\"\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def xavier_init(c1, c2, w=1,h=1,fc=False):\n",
    "    fan_1 = c2 * w * h\n",
    "    fan_2 = c1 * w * h\n",
    "    ratio = np.sqrt(6.0 / (fan_1 + fan_2))\n",
    "    params = ratio * (2 * np.random.random((c1, c2, w, h)) - 1)\n",
    "    if fc:\n",
    "        params = params.reshape(c1, c2)\n",
    "    return params\n",
    "\n",
    "class My_LSTM(object):\n",
    "    def __init__(self, x_size, hidden_size, num_classes):\n",
    "        self.lr = learning_rate\n",
    "        self.seq_length = seq_length\n",
    "        self.input_size = x_size + hidden_size\n",
    "        \n",
    "        self.W_f = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_i = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_g = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_g = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_o = xavier_init(hidden_size, self.input_size, fc=True)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        self.W_fc = xavier_init(num_classes, hidden_size, fc=True)\n",
    "        self.b_fc = np.zeros((num_classes, 1))\n",
    "        \n",
    "        self.mW_f = np.zeros_like(self.W_f)\n",
    "        self.mb_f = np.zeros_like(self.b_f)\n",
    "        \n",
    "        self.mW_i = np.zeros_like(self.W_i)\n",
    "        self.mb_i = np.zeros_like(self.b_i)\n",
    "        \n",
    "        self.mW_g = np.zeros_like(self.W_g)\n",
    "        self.mb_g = np.zeros_like(self.b_g)\n",
    "        \n",
    "        self.mW_o = np.zeros_like(self.W_o)\n",
    "        self.mb_o = np.zeros_like(self.b_o)\n",
    "        \n",
    "        self.mW_fc = np.zeros_like(self.W_fc)\n",
    "        self.mb_fc = np.zeros_like(self.b_fc)\n",
    "        \n",
    "        self.X = {}\n",
    "        self.F = {}\n",
    "        self.F_A = {}\n",
    "        \n",
    "        self.I = {}\n",
    "        self.I_A = {}\n",
    "        \n",
    "        self.G = {}\n",
    "        self.G_A = {}\n",
    "        \n",
    "        self.O = {}\n",
    "        self.O_A = {}\n",
    "        \n",
    "        self.C = {}\n",
    "        self.C_A = {}\n",
    "        self.H = {}\n",
    "        \n",
    "    def forward(self, x, hprev, cprev):\n",
    "        self.X = {}\n",
    "        self.F = {}\n",
    "        self.F_A = {}\n",
    "        \n",
    "        self.I = {}\n",
    "        self.I_A = {}\n",
    "        \n",
    "        self.G = {}\n",
    "        self.G_A = {}\n",
    "        \n",
    "        self.O = {}\n",
    "        self.O_A = {}\n",
    "        \n",
    "        self.C = {}\n",
    "        self.C_A = {}\n",
    "        self.H = {}\n",
    "        \n",
    "        self.H[-1] = np.copy(hprev)\n",
    "        self.C[-1] = np.copy(cprev)\n",
    "        \n",
    "        for t in range(self.seq_length):\n",
    "            self.X[t] = np.concatenate((self.H[t-1], x[t].T), axis = 0)\n",
    "            \n",
    "            self.F[t] = self.W_f @ self.X[t] + self.b_f\n",
    "            self.F_A[t] = self.sigmoid(self.F[t])\n",
    "            \n",
    "            self.I[t] = self.W_i @ self.X[t] + self.b_i\n",
    "            self.I_A[t] = self.sigmoid(self.I[t])\n",
    "            \n",
    "            self.G[t] = self.W_g @ self.X[t] + self.b_g\n",
    "            self.G_A[t] = np.tanh(self.G[t])\n",
    "            \n",
    "            self.C[t] = self.F_A[t] * self.C[t - 1] + self.I_A[t] * self.G_A[t]\n",
    "            self.C_A[t] = np.tanh(self.C[t])\n",
    "            \n",
    "            self.O[t] = self.W_o @ self.X[t] + self.b_o\n",
    "            self.O_A[t] = self.sigmoid(self.O[t])\n",
    "            \n",
    "            self.H[t] = self.O_A[t] * self.C_A[t]\n",
    "            \n",
    "        output = self.W_fc @ self.H[self.seq_length - 1] + self.b_fc\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dY):\n",
    "        dW_f, db_f = np.zeros_like(self.W_f), np.zeros_like(self.b_f)\n",
    "        dW_i, db_i = np.zeros_like(self.W_i), np.zeros_like(self.b_i)\n",
    "        dW_g, db_g = np.zeros_like(self.W_g), np.zeros_like(self.b_g)\n",
    "        dW_o, db_o = np.zeros_like(self.W_o), np.zeros_like(self.b_o)\n",
    "        dW_fc, db_fc = np.zeros_like(self.W_fc), np.zeros_like(self.b_fc)\n",
    "        \n",
    "        dH_next = np.zeros_like(self.H[0])\n",
    "        dC_next = np.zeros_like(self.C[0])\n",
    "        \n",
    "        dW_fc = dY @ self.H[self.seq_length - 1].T\n",
    "        db_fc = dY\n",
    "        \n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dh = self.W_fc.T @ dY + dH_next\n",
    "            \n",
    "            dO_A = dh * self.C_A[t]\n",
    "            dO = dO_A * (self.O_A[t] * (1 - self.O_A[t]))\n",
    "            dW_o += dO @ self.X[t].T\n",
    "            db_o += dO\n",
    "            \n",
    "            dC_A = self.O_A[t] * dh\n",
    "            dC = dC_A * (1 - self.C_A[t] ** 2) + dC_next\n",
    "            \n",
    "            dF_A = dC * self.C[t - 1]\n",
    "            dI_A = dC * self.G_A[t]\n",
    "            dG_A = self.I_A[t] * dC\n",
    "            dC_next = self.F_A[t] * dC\n",
    "            \n",
    "            dF = dF_A * (self.F_A[t] * (1 - self.F_A[t]))\n",
    "            dW_f += dF @ self.X[t].T\n",
    "            db_f += dF\n",
    "            \n",
    "            dI = dI_A * (self.I_A[t] * (1 - self.I_A[t]))\n",
    "            dW_i += dI @ self.X[t].T\n",
    "            db_i += dI\n",
    "            \n",
    "            dG = dG_A * (1 - self.G_A[t] ** 2)\n",
    "            dW_g += dG @ self.X[t].T\n",
    "            db_g += dG\n",
    "            \n",
    "            dX = self.W_f.T @ dF + self.W_i.T @ dI + self.W_g.T @ dG + self.W_o.T @ dO\n",
    "            dH_next = dX[:hidden_size, :]\n",
    "        \n",
    "        gradients = [dW_f, db_f, dW_i, db_i, dW_g, db_g, dW_o, db_o, dW_fc, db_fc]\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def optimizer_step(self, gradients):\n",
    "        for dparam in gradients:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "        for param, dparam, mem in zip(\n",
    "            [self.W_f, self.b_f, self.W_i, self.b_i, self.W_g, self.b_g, self.W_o, self.b_o, self.W_fc, self.b_fc],\n",
    "            gradients,\n",
    "            [self.mW_f, self.mb_f, self.mW_i, self.mb_i, self.mW_g, self.mb_g, self.mW_o, self.mb_o, self.mW_fc, self.mb_fc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.lr * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "    def cross_entropy_loss(self, outputs, labels):\n",
    "        Y = self.softmax(outputs)\n",
    "        loss = -np.log(Y) * self.one_hot_vector(Y, labels)\n",
    "        return Y, loss\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e)\n",
    "    \n",
    "    def deriv_softmax(self, Y, labels):\n",
    "        dY = np.copy(Y)\n",
    "        for i in range(len(labels)):\n",
    "            dY[labels[i]][i] -= 1\n",
    "        return dY\n",
    "    \n",
    "    def one_hot_vector(self, Y, labels):\n",
    "        out = np.zeros_like(Y)\n",
    "        for i in range(len(labels)):\n",
    "            out[labels[i]][i] = 1\n",
    "        return out\n",
    "    \n",
    "    def predict(self, outputs):\n",
    "        return np.argmax(self.softmax(outputs), 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = My_LSTM(input_size, hidden_size, num_classes)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "iter_loss = 0\n",
    "interval = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "        \n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        cprev = np.zeros((hidden_size, 1))\n",
    "        outputs = model.forward(images, hprev, cprev)\n",
    "        Y, loss = model.cross_entropy_loss(outputs, labels)\n",
    "        gradients = model.backward(model.deriv_softmax(Y, labels))\n",
    "        model.optimizer_step(gradients)\n",
    "        iter_loss += np.sum(loss)\n",
    "        if(i + 1) % interval == 0:\n",
    "            print(\"epoch {}/{} iter {}/{} loss {:.4f}\".format(epoch + 1, num_epochs, i + 1, total_step, iter_loss / interval))\n",
    "            iter_loss = 0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1/2 iter 1000/60000 loss 1.9428\n",
      "epoch 1/2 iter 2000/60000 loss 1.3351\n",
      "epoch 1/2 iter 3000/60000 loss 1.0977\n",
      "epoch 1/2 iter 4000/60000 loss 0.8128\n",
      "epoch 1/2 iter 5000/60000 loss 0.6897\n",
      "epoch 1/2 iter 6000/60000 loss 0.6370\n",
      "epoch 1/2 iter 7000/60000 loss 0.5621\n",
      "epoch 1/2 iter 8000/60000 loss 0.5718\n",
      "epoch 1/2 iter 9000/60000 loss 0.5631\n",
      "epoch 1/2 iter 10000/60000 loss 0.5183\n",
      "epoch 1/2 iter 11000/60000 loss 0.4741\n",
      "epoch 1/2 iter 12000/60000 loss 0.4034\n",
      "epoch 1/2 iter 13000/60000 loss 0.3996\n",
      "epoch 1/2 iter 14000/60000 loss 0.3660\n",
      "epoch 1/2 iter 15000/60000 loss 0.3830\n",
      "epoch 1/2 iter 16000/60000 loss 0.3835\n",
      "epoch 1/2 iter 17000/60000 loss 0.4276\n",
      "epoch 1/2 iter 18000/60000 loss 0.3159\n",
      "epoch 1/2 iter 19000/60000 loss 0.3816\n",
      "epoch 1/2 iter 20000/60000 loss 0.3607\n",
      "epoch 1/2 iter 21000/60000 loss 0.2750\n",
      "epoch 1/2 iter 22000/60000 loss 0.3527\n",
      "epoch 1/2 iter 23000/60000 loss 0.2826\n",
      "epoch 1/2 iter 24000/60000 loss 0.2794\n",
      "epoch 1/2 iter 25000/60000 loss 0.3188\n",
      "epoch 1/2 iter 26000/60000 loss 0.3068\n",
      "epoch 1/2 iter 27000/60000 loss 0.3300\n",
      "epoch 1/2 iter 28000/60000 loss 0.2994\n",
      "epoch 1/2 iter 29000/60000 loss 0.3113\n",
      "epoch 1/2 iter 30000/60000 loss 0.2598\n",
      "epoch 1/2 iter 31000/60000 loss 0.2505\n",
      "epoch 1/2 iter 32000/60000 loss 0.2824\n",
      "epoch 1/2 iter 33000/60000 loss 0.2578\n",
      "epoch 1/2 iter 34000/60000 loss 0.2800\n",
      "epoch 1/2 iter 35000/60000 loss 0.2990\n",
      "epoch 1/2 iter 36000/60000 loss 0.2910\n",
      "epoch 1/2 iter 37000/60000 loss 0.2987\n",
      "epoch 1/2 iter 38000/60000 loss 0.2302\n",
      "epoch 1/2 iter 39000/60000 loss 0.2452\n",
      "epoch 1/2 iter 40000/60000 loss 0.2449\n",
      "epoch 1/2 iter 41000/60000 loss 0.2054\n",
      "epoch 1/2 iter 42000/60000 loss 0.2837\n",
      "epoch 1/2 iter 43000/60000 loss 0.2095\n",
      "epoch 1/2 iter 44000/60000 loss 0.2439\n",
      "epoch 1/2 iter 45000/60000 loss 0.2585\n",
      "epoch 1/2 iter 46000/60000 loss 0.1939\n",
      "epoch 1/2 iter 47000/60000 loss 0.2380\n",
      "epoch 1/2 iter 48000/60000 loss 0.2241\n",
      "epoch 1/2 iter 49000/60000 loss 0.2331\n",
      "epoch 1/2 iter 50000/60000 loss 0.2700\n",
      "epoch 1/2 iter 51000/60000 loss 0.2229\n",
      "epoch 1/2 iter 52000/60000 loss 0.2461\n",
      "epoch 1/2 iter 53000/60000 loss 0.1982\n",
      "epoch 1/2 iter 54000/60000 loss 0.2603\n",
      "epoch 1/2 iter 55000/60000 loss 0.2251\n",
      "epoch 1/2 iter 56000/60000 loss 0.2502\n",
      "epoch 1/2 iter 57000/60000 loss 0.1994\n",
      "epoch 1/2 iter 58000/60000 loss 0.2727\n",
      "epoch 1/2 iter 59000/60000 loss 0.1782\n",
      "epoch 1/2 iter 60000/60000 loss 0.2157\n",
      "epoch 2/2 iter 1000/60000 loss 0.1932\n",
      "epoch 2/2 iter 2000/60000 loss 0.1770\n",
      "epoch 2/2 iter 3000/60000 loss 0.2036\n",
      "epoch 2/2 iter 4000/60000 loss 0.1870\n",
      "epoch 2/2 iter 5000/60000 loss 0.1795\n",
      "epoch 2/2 iter 6000/60000 loss 0.1919\n",
      "epoch 2/2 iter 7000/60000 loss 0.2029\n",
      "epoch 2/2 iter 8000/60000 loss 0.2077\n",
      "epoch 2/2 iter 9000/60000 loss 0.2009\n",
      "epoch 2/2 iter 10000/60000 loss 0.2056\n",
      "epoch 2/2 iter 11000/60000 loss 0.1893\n",
      "epoch 2/2 iter 12000/60000 loss 0.2036\n",
      "epoch 2/2 iter 13000/60000 loss 0.1827\n",
      "epoch 2/2 iter 14000/60000 loss 0.2134\n",
      "epoch 2/2 iter 15000/60000 loss 0.1573\n",
      "epoch 2/2 iter 16000/60000 loss 0.1807\n",
      "epoch 2/2 iter 17000/60000 loss 0.1943\n",
      "epoch 2/2 iter 18000/60000 loss 0.2312\n",
      "epoch 2/2 iter 19000/60000 loss 0.2005\n",
      "epoch 2/2 iter 20000/60000 loss 0.1848\n",
      "epoch 2/2 iter 21000/60000 loss 0.1629\n",
      "epoch 2/2 iter 22000/60000 loss 0.1969\n",
      "epoch 2/2 iter 23000/60000 loss 0.1685\n",
      "epoch 2/2 iter 24000/60000 loss 0.1790\n",
      "epoch 2/2 iter 25000/60000 loss 0.1745\n",
      "epoch 2/2 iter 26000/60000 loss 0.1855\n",
      "epoch 2/2 iter 27000/60000 loss 0.1897\n",
      "epoch 2/2 iter 28000/60000 loss 0.1909\n",
      "epoch 2/2 iter 29000/60000 loss 0.1928\n",
      "epoch 2/2 iter 30000/60000 loss 0.2036\n",
      "epoch 2/2 iter 31000/60000 loss 0.1755\n",
      "epoch 2/2 iter 32000/60000 loss 0.2220\n",
      "epoch 2/2 iter 33000/60000 loss 0.1910\n",
      "epoch 2/2 iter 34000/60000 loss 0.1548\n",
      "epoch 2/2 iter 35000/60000 loss 0.1417\n",
      "epoch 2/2 iter 36000/60000 loss 0.2206\n",
      "epoch 2/2 iter 37000/60000 loss 0.1503\n",
      "epoch 2/2 iter 38000/60000 loss 0.1668\n",
      "epoch 2/2 iter 39000/60000 loss 0.1686\n",
      "epoch 2/2 iter 40000/60000 loss 0.1963\n",
      "epoch 2/2 iter 41000/60000 loss 0.1717\n",
      "epoch 2/2 iter 42000/60000 loss 0.1348\n",
      "epoch 2/2 iter 43000/60000 loss 0.1602\n",
      "epoch 2/2 iter 44000/60000 loss 0.1707\n",
      "epoch 2/2 iter 45000/60000 loss 0.2051\n",
      "epoch 2/2 iter 46000/60000 loss 0.1568\n",
      "epoch 2/2 iter 47000/60000 loss 0.1491\n",
      "epoch 2/2 iter 48000/60000 loss 0.1670\n",
      "epoch 2/2 iter 49000/60000 loss 0.1569\n",
      "epoch 2/2 iter 50000/60000 loss 0.1577\n",
      "epoch 2/2 iter 51000/60000 loss 0.1569\n",
      "epoch 2/2 iter 52000/60000 loss 0.1885\n",
      "epoch 2/2 iter 53000/60000 loss 0.1598\n",
      "epoch 2/2 iter 54000/60000 loss 0.1641\n",
      "epoch 2/2 iter 55000/60000 loss 0.1220\n",
      "epoch 2/2 iter 56000/60000 loss 0.1634\n",
      "epoch 2/2 iter 57000/60000 loss 0.1519\n",
      "epoch 2/2 iter 58000/60000 loss 0.1519\n",
      "epoch 2/2 iter 59000/60000 loss 0.1488\n",
      "epoch 2/2 iter 60000/60000 loss 0.1363\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.reshape(seq_length, batch_size, input_size).detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "    \n",
    "    hprev = np.zeros((hidden_size, 1))\n",
    "    cprev = np.zeros((hidden_size, 1))\n",
    "    outputs = model.forward(images, hprev, cprev)\n",
    "    pred = model.predict(outputs)\n",
    "    total += labels.shape[0]\n",
    "    correct += (pred == labels).sum().item()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 95.73 %\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('MyTorch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "10a79c3b7134e4f43fad46b95b4586eb332955eb49b164ff904fc2275497eef9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}